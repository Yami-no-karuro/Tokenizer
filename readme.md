# Rust - Tokenizer

## Simple Tokenizer, in Rust

### Intro

A **Tokenizer** is a fundamental component in **Natural Language Processing** (NLP) and text analysis. 
Its main purpose is to split raw text into smaller units called **Tokens**, which can be words, subwords, or even characters. 
These tokens serve as the basic elements for further linguistic or computational processing, such as parsing, vectorization, or feeding into machine learning models.  
(More on [Lexical Analysis](https://en.wikipedia.org/wiki/Lexical_analysis) and [NPL](https://en.wikipedia.org/wiki/Natural_language_processing) on [Wikipedia](https://en.wikipedia.org/))

